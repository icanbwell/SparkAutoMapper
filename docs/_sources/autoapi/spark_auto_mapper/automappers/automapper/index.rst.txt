:mod:`spark_auto_mapper.automappers.automapper`
===============================================

.. py:module:: spark_auto_mapper.automappers.automapper


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   spark_auto_mapper.automappers.automapper.AutoMapper



.. data:: TEMPORARY_KEY
   :annotation: = __row_id

   

.. class:: AutoMapper(keys: Optional[List[str]] = None, view: Optional[str] = None, source_view: Optional[str] = None, keep_duplicates: bool = False, drop_key_columns: bool = True, checkpoint_after_columns: Optional[int] = None, checkpoint_path: Optional[Union[(str, pathlib.Path)]] = None, reuse_existing_view: bool = False, use_schema: bool = True, include_extension: bool = False, include_null_properties: bool = False, use_single_select: bool = True, verify_row_count: bool = True, skip_schema_validation: List[str] = ['extension'], skip_if_columns_null_or_empty: Optional[List[str]] = None, keep_null_rows: bool = False, filter_by: Optional[str] = None, logger: Optional[logging.Logger] = None, check_schema_for_all_columns: bool = False, copy_all_unmapped_properties: bool = False, copy_all_unmapped_properties_exclude: Optional[List[str]] = None, log_level: Optional[Union[(int, str)]] = None)


   Bases: :class:`spark_auto_mapper.automappers.container.AutoMapperContainer`

   This is the main AutoMapper class.  It is used to create an AutoMapper and then you can add mappings.

   Creates an AutoMapper

   :param keys: joining keys
   :param view: view to return
   :param source_view: where to load the data from
   :param keep_duplicates: whether to leave duplicates at the end
   :param drop_key_columns: whether to drop the key columns at the end
   :param checkpoint_after_columns: checkpoint after how many columns have been processed
   :param checkpoint_path: Path where to store the checkpoints
   :param reuse_existing_view: If view already exists, whether to reuse it or create a new one
   :param use_schema: apply schema to columns
   :param include_extension: By default we don't include extension elements since they take up a lot of schema.
           If you're using extensions then set this
   :param include_null_properties: If you want to include null properties
   :param use_single_select: This is a faster way to run the AutoMapper since it will select
           all the columns at once.
           However this makes it harder to debug since you don't know what column failed
   :param verify_row_count: verifies that the count of rows remains the same before and after the transformation
   :param skip_schema_validation: skip schema checks on these columns
   :param skip_if_columns_null_or_empty: skip creating the record if any of these columns are null or empty
   :param keep_null_rows: whether to keep the null rows instead of removing them
   :param filter_by: (Optional) SQL expression that is used to filter
   :param copy_all_unmapped_properties: copy any property that is not explicitly mapped
   :param copy_all_unmapped_properties_exclude: exclude these columns when copy_all_unmapped_properties is set
   :param logger: logger used to log informational messages

   .. method:: _transform_with_data_frame_single_select(self, df: pyspark.sql.DataFrame, source_df: pyspark.sql.DataFrame, keys: List[str]) -> pyspark.sql.DataFrame

      This functions transforms the data frame using the mappings in a single select command


   .. method:: transform_with_data_frame(self, df: pyspark.sql.DataFrame, source_df: Optional[pyspark.sql.DataFrame], keys: List[str]) -> pyspark.sql.DataFrame


   .. method:: get_message_for_exception(*, column_name: str, df: pyspark.sql.DataFrame, e: Exception, source_df: pyspark.sql.DataFrame, column_values: Optional[List[Any]]) -> str
      :staticmethod:


   .. method:: transform(self, df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame


   .. method:: register_child(self, dst_column: str, child: spark_auto_mapper.automappers.automapper_base.AutoMapperBase) -> None


   .. method:: columns(self, **kwargs: spark_auto_mapper.type_definitions.defined_types.AutoMapperAnyDataType) -> spark_auto_mapper.automappers.automapper.AutoMapper


   .. method:: complex(self, entity: spark_auto_mapper.data_types.complex.complex_base.AutoMapperDataTypeComplexBase) -> spark_auto_mapper.automappers.automapper.AutoMapper


   .. method:: __repr__(self) -> str

      Display for debugger

      :return: string representation for debugger


   .. method:: to_debug_string(self, source_df: Optional[pyspark.sql.DataFrame] = None) -> str

      Displays the automapper as a string

      :param source_df: (Optional) source data frame
      :return: string representation


   .. method:: column_specs(self) -> Dict[(str, pyspark.sql.Column)]
      :property:

      Useful to show in debugger

      :return dictionary of column specs


   .. method:: __str__(self) -> str

      Return str(self).



