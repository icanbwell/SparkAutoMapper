:py:mod:`spark_auto_mapper.data_types.data_type_base`
=====================================================

.. py:module:: spark_auto_mapper.data_types.data_type_base


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   spark_auto_mapper.data_types.data_type_base.AutoMapperDataTypeBase




.. py:class:: AutoMapperDataTypeBase

   Base class for all Automapper data types

   .. py:method:: get_column_spec(self, source_df, current_column)
      :abstractmethod:

      Gets the column spec for this automapper data type

      :param source_df: source data frame in case the automapper type needs that data to decide what to do
      :param current_column: (Optional) this is set when we are inside an array


   .. py:method:: get_value(self, value, source_df, current_column)

      Gets the value for this automapper

      :param value: current value
      :param source_df: source data frame in case the automapper type needs that data to decide what to do
      :param current_column: (Optional) this is set when we are inside an array


   .. py:method:: include_null_properties(self, include_null_properties)


   .. py:method:: transform(self, value)

      transforms a column into another type or struct


      :param value: Complex or Simple Type to create for each item in the array
      :return: a transform automapper type
      :example: A.column("last_name").transform(A.complex(bar=A.field("value"), bar2=A.field("system")))


   .. py:method:: select(self, value)

      transforms a column into another type or struct


      :param value: Complex or Simple Type to create for each item in the array
      :return: a transform automapper type
      :example: A.column("last_name").select(A.complex(bar=A.field("value"), bar2=A.field("system")))


   .. py:method:: filter(self, func)

      filters an array column


      :param func: func to create type or struct
      :return: a filter automapper type
      :example: A.column("last_name").filter(lambda x: x["use"] == lit("usual")
      )


   .. py:method:: split_by_delimiter(self, delimiter)

      splits a text column by the delimiter to create an array


      :param delimiter: delimiter
      :return: a split_by_delimiter automapper type
      :example: A.column("last_name").split_by_delimiter("|")


   .. py:method:: select_one(self, value)

      selects first item from array


      :param value: Complex or Simple Type to create for each item in the array
      :return: a transform automapper type
      :example: A.column("identifier").select_one(A.field("_.value"))


   .. py:method:: first(self)

      returns the first element in array

      :return: a filter automapper type
      :example: A.column("identifier").select(A.field("_.value")).first()


   .. py:method:: expression(self, value)

      Specifies that the value parameter should be executed as a sql expression in Spark

      :param value: sql
      :return: an expression automapper type
      :example: A.column("identifier").select(A.field("_.value")).first()


   .. py:method:: current(self)

      Specifies to use the current item

      :return: A column automapper type


   .. py:method:: field(self, value)

      Specifies that the value parameter should be used as a field name

      :param value: name of column
      :return: A column automapper type


   .. py:method:: flatten(self)

      creates a single array from an array of arrays.
      If a structure of nested arrays is deeper than two levels, only one level of nesting is removed.
      source: http://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html#flatten

      :return: a flatten automapper type


   .. py:method:: to_array(self)

      converts single element into an array


      :return: an automapper type


   .. py:method:: concat(self, list2)

      concatenates two arrays or strings


      :param list2:
      :return: a filter automapper type


   .. py:method:: to_float(self)

      Converts column to float

      :return: a float automapper type


   .. py:method:: to_date(self, formats = None)

      Converts a value to date only
      For datetime use the datetime mapper type


      :param formats: (Optional) formats to use for trying to parse the value otherwise uses:
                      y-M-d,
                      yyyyMMdd,
                      M/d/y


   .. py:method:: to_datetime(self, formats = None)

      Converts the value to a timestamp type in Spark


      :param formats: (Optional) formats to use for trying to parse the value otherwise uses Spark defaults


   .. py:method:: to_amount(self)

      Specifies the value should be used as an amount

      :return: an amount automapper type


   .. py:method:: to_boolean(self)

      Specifies the value should be used as a boolean

      :return: a boolean automapper type


   .. py:method:: to_number(self)

      Specifies value should be used as a number

      :return: a number automapper type


   .. py:method:: to_text(self)

      Specifies that the value parameter should be used as a literal text

      :return: a text automapper type


   .. py:method:: join_using_delimiter(self, delimiter)

      Joins an array and forms a string using the delimiter

      :param delimiter: string to use as delimiter
      :return: a join_using_delimiter automapper type


   .. py:method:: get_schema(self, include_extension)


   .. py:method:: to_date_format(self, format_)

      Converts a date or time into string


      :param format_: format to use for trying to parse the value otherwise uses:
                      y-M-d
                      yyyyMMdd
                      M/d/y


   .. py:method:: to_null_if_empty(self)

      returns null if the column is an empty string


      :return: an automapper type


   .. py:method:: regex_replace(self, pattern, replacement)

      Replace all substrings of the specified string value that match regexp with replacement.

      :param pattern: pattern to search for
      :param replacement: string to replace with
      :return: a regex_replace automapper type


   .. py:method:: sanitize(self, pattern = '[^\\w\\r\\n\\t _.,!\\"\'/$-]', replacement = ' ')

             Replaces all "non-normal" characters with specified replacement

             By default, We're using the FHIR definition of valid string
             (except /S does not seem to work properly in Spark)
             https://www.hl7.org/fhir/datatypes.html#string
             Valid characters are (regex='[ 
             \S]'):

             \S - Any character that is not a whitespace character

                - space

             
      - carriage return

             
      - line feed

                      - tab

             :param pattern: regex pattern of characters to replace
             :param replacement: (Optional) string to replace with.  Defaults to space.
             :return: a regex_replace automapper type
             


   .. py:method:: if_exists(self, if_exists = None, if_not_exists = None)

      returns column if it exists else returns null


      :return: an automapper type


   .. py:method:: cast(self, type_)

      casts columns to type

      :param type_: type to cast to
      :return: an automapper type


   .. py:method:: __add__(self, other)



